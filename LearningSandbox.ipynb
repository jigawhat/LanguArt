{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alfew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from Learning import *\n",
    "from Graphing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76.92, 63.46, 55.77, 67.31, 76.92, 53.85, 73.08]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, scorers = models_c, scorers_c[:1]\n",
    "Y_labels = Y_labels_de[1:] # All except 'inc', which is binary\n",
    "# Y_labels = Y_labels_de\n",
    "# Y_labels = ['dif', 'nrd', 'skt'] # Sketchability\n",
    "# Y_labels = ['dif', 'nrd', 'skt', 'vis'] # Visuality\n",
    "# Y_labels = ['vis', 'phy', 'obj'] # Physicality\n",
    "df, X, Y = load_data(Y_labels)\n",
    "\n",
    "# Convert for classification\n",
    "threshold = 5.0\n",
    "Y = (Y > threshold).astype(np.int)\n",
    "\n",
    "# Class balance percentages\n",
    "[round(100 * max(np.mean(Y[:, i]), 1- np.mean(Y[:, i])), 2) for i in range(len(Y_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multi-task models on interval_size-sample increments of data, up to the whole dataset\n",
    "interval_size = 26\n",
    "ns_samples = list(np.arange(0, X.shape[0], interval_size)[1:-1]) + [ X.shape[0] ]\n",
    "\n",
    "# Take ns_eval_samples samples of random x for each increment, to get accurate results at low n\n",
    "ns_eval_samples = [10] * len(ns_samples)\n",
    "print(sum(sum(Y == 1))/sum(sum(np.ones(Y.shape))))\n",
    "X.shape, Y.shape, ns_samples, ns_eval_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1, 10, (52, 7))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross validation prediction\n",
    "Y_pr = do_cv_learning_curve(X, Y, models, ns_samples, ns_eval_samples, cv=3) # Result includes sample indices used\n",
    "save_ld((X, Y, Y_pr, Y_labels, models, scorers, ns_samples, ns_eval_samples), \"preds_52_\" + str(time.time()))\n",
    "# X, Y, Y_pr, Y_labels, models, scorers, ns_samples, ns_eval_samples = load_learning_data(\"preds_s_1e_1513502596.5260706\")\n",
    "len(Y_pr), len(Y_pr[-1]), len(Y_pr[-1][-1]), Y_pr[-1][-1][-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoostClassifier\n",
      "'multitask_acc: [0.767]'\n",
      "\tRandomForestClassifier\n",
      "'multitask_acc: [0.771]'\n",
      "\tDecisionTreeClassifier\n",
      "'multitask_acc: [0.794]'\n",
      "\tKNeighborsClassifier\n",
      "'multitask_acc: [0.6]'\n",
      "\tSVC\n",
      "'multitask_acc: [0.735]'\n",
      "\tGaussianProcessClassifier\n",
      "'multitask_acc: [0.744]'\n",
      "\tLogisticRegression\n",
      "'multitask_acc: [0.783]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "scores = score_learning_curve(Y, Y_pr, models, ns_samples, ns_eval_samples, scorers)\n",
    "print_learning_curve_mean_scores(models, scorers, scores, Y_labels, [\"skt\"]); print()\n",
    "# graph_learning_curve(models, scorers, scores, ns_samples, Y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1, 10, (52, 7))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For debug: convert x data into random numbers & try to learn\n",
    "X = np.random.random((X.shape[0], 1000))\n",
    "Y_pr = do_cv_learning_curve(X, Y, models, ns_samples, ns_eval_samples, cv=3) # Result includes sample indices used\n",
    "len(Y_pr), len(Y_pr[-1]), len(Y_pr[-1][-1]), Y_pr[-1][-1][-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoostClassifier\n",
      "'multitask_acc: [0.713]'\n",
      "\tRandomForestClassifier\n",
      "'multitask_acc: [0.744]'\n",
      "\tDecisionTreeClassifier\n",
      "'multitask_acc: [0.733]'\n",
      "\tKNeighborsClassifier\n",
      "'multitask_acc: [0.592]'\n",
      "\tSVC\n",
      "'multitask_acc: [0.692]'\n",
      "\tGaussianProcessClassifier\n",
      "'multitask_acc: [0.777]'\n",
      "\tLogisticRegression\n",
      "'multitask_acc: [0.75]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "scores = score_learning_curve(Y, Y_pr, models, ns_samples, ns_eval_samples, scorers)\n",
    "print_learning_curve_mean_scores(models, scorers, scores, Y_labels, [\"skt\"]); print()\n",
    "# graph_learning_curve(models, scorers, scores, ns_samples, Y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 'bounces'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-34f65a2f7aa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Inspect errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word: '\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Actual: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect errors\n",
    "print(\"Word: '\" + df.index[11] + \"'\")\n",
    "print(\"Prediction: \" + str([round(y, 3) for y in results[0][-1][0][0][11]]))\n",
    "print(\"Actual: \" + str(Y[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data for new word prediction\n",
    "ngrams_db = load_ngram_counts()             # Load ngram counts database\n",
    "wrep_lib = WordRepLibrary(lib_paths[-1])    # Load word representation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict for some random new words\n",
    "n_pred = 100\n",
    "i_seen = map(int, list(df[\"lib_i\"]))\n",
    "model = models_lib[models[-1]]\n",
    "model.fit(X, Y)\n",
    "print(\"         \", Y_labels)\n",
    "for i in range(n_pred):\n",
    "    word, word_i, x = wrep_lib.get_new_word(i_seen)\n",
    "    while word not in ngrams_db.index or not valid_word(word, verbose=False):\n",
    "        word, word_i, x = wrep_lib.get_new_word()\n",
    "    x += list(ngrams_db.loc[word]) + [ google_search_count(word) ]\n",
    "    y_pred = model.predict(np.atleast_2d(x))\n",
    "    print(word + \": \" + str(format_numlist(y_pred[0], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
