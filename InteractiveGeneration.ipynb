{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, ReformerModelWithLMHead, get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *\n",
    "device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ###   Options   ###\n",
    "\n",
    "model_name = \"ernst_one\"             # First, use a mean pooling of the claim and chained evidence\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set  (list sets)\n",
    "sample_test_n = 25                   # Number of randomly generated prompts for each sample when testing model\n",
    "log_period_batches = 25              # Batches per iteration\n",
    "learning_rate = 5e-5                 # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 8                 # Samples per batch\n",
    "N_train_batches = int(1e7 / bsz)     # Total number of batches to show model\n",
    "min_nw, max_nw = 0.17, 0.8           # Minimum and maximum fraction of list to keep when truncating\n",
    "max_listlen = 20                     # Maximum number of words in the list when creating a prompt (at least prior to * max_nw)\n",
    "lidstone_e = 0.0                     # Smoothing for possible words/subwords which are not in the missing list words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [(tokenizer.encode(prompt), \"types of\" in prompt) for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk, using nw_draw = 15. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p):  # prmt = prompt tokens, p = list phrases tokens list\n",
    "    tkzs, sent, tkix = [], [], 0\n",
    "    incl_words = np.random.choice(len(p), min(max_listlen, len(p)), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc = p[phz_i]\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent += phz_enc\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(round(tkix * min_nw), round(tkix * max_nw))\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if enc[:trunc_n] == partial_phr]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = 1 / norm, np.tile(lidstone_e / N_tokens, N_tokens)\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return np.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def gen_samples_uniform(xcp, xcs, xp, nw, verbose=False):  # Weight testing samples (word lists) uniformly\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        for m in range(nw):\n",
    "            prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "            cat_ix = np.random.randint(len(cp))\n",
    "            x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "            sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0:\n",
    "                sys_print(\"\\rDone: \" + str(j))\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        sqlens.append(sqlen)\n",
    "    if verbose: sys_print(\"\\rDone: \" + str(j) + \", finished!\\n\")\n",
    "    return xs, ys, sqlens\n",
    "def gen_samples(xcp, xcs, xp, n):  # Maximise training batch diversity by randomly sampling the word lists\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets = len(xcp)\n",
    "    for m in range(n):\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "        cat_ix = np.random.randint(len(cp))\n",
    "        x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "        sqlens.append(len(x_))\n",
    "    return xs, ys, sqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Done: 50, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_wordlists, N_test, replace=False)\n",
    "test_idx = np.array([3, 7])\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens = gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, sample_test_n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ld((test_xs, test_ys, test_sqlens), \"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_xs, test_ys, test_sqlens = load_ld(\"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens):\n",
    "    max_len = max(sqlens)\n",
    "    xs = pt.tensor(np.vstack([np.pad(x, (0, max_len - len(x)), constant_values=50256) for x in xs])).to(device)\n",
    "    ys = pt.tensor(np.vstack(ys)).to(device)\n",
    "    sqlens = pt.tensor(np.asarray(sqlens)).to(device)\n",
    "    return xs, ys, sqlens\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train\n",
    "    xs, ys, sqlens = gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz)\n",
    "    return adapt_form(xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2SeqClas\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2-large',\n",
    "    output_hidden_states=True, output_attentions=True, \n",
    "    cache_dir=\"models/pretrained/GPT2SeqClas\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "n_embd = model.config.n_embd\n",
    "model = nn.DataParallel(model, device_ids=list(range(pt.cuda.device_count()))).to(device)\n",
    "print(\"GPT2 loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = nn.Linear(n_embd, len(tokenizer), bias=False)\n",
    "llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(device)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()\n",
    "# bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(device)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(batch_size)\n",
    "    x_batch, y_batch, sqlens_batch = x_batch.to(device), y_batch.to(device), sqlens_batch.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    outputs = model(x_batch.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens_batch - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1], 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    \n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.sum().backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens):\n",
    "    global model, llayer\n",
    "    \n",
    "    x, sqlens = x.to(device), sqlens.to(device)\n",
    "    outputs = model(x.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1], 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    return logits\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "    \n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_\n",
    "\n",
    "# top_next = [self.tokenizer.decode(i.item()).strip() for i in probs.topk(k)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        gc.collect()\n",
    "        b_loss, b_accuracy = train_step()\n",
    "        if verbose:\n",
    "            sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                      ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete. ')\n",
    "        iter_loss.append(b_loss)\n",
    "        iter_accuracy.append(b_accuracy)\n",
    "        \n",
    "        if (batch_i - 1) % log_period_batches == 0:  # Test on test set\n",
    "            loss, accuracy = [], []\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i])\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + 1)]\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                print('\\n' + test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]))\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if test_a > best_acc:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "                         \"llayer\": llayer.state_dict(),\n",
    "#                          \"softrmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(\"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 : 0.0 0.0 loss: 0.73000735 0.7074908 Best: 0 inf idx: -1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.78 GiB total capacity; 14.03 GiB already allocated; 8.00 MiB free; 14.36 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miterate_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-caa1f024ffe8>\u001b[0m in \u001b[0;36miterate_training\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
      "\u001b[0;32m<ipython-input-13-8d3953488552>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.78 GiB total capacity; 14.03 GiB already allocated; 8.00 MiB free; 14.36 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load a model\n",
    "# gc.collect()\n",
    "# checkpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# llayer.load_state_dict(checkpoint['llayer'])\n",
    "# bcewl_loss.load_state_dict(checkpoint['bcewl_loss'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_next_token(sent, top_k=-1, top_p=0.9, temperature=1.0):\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    x = pt.tensor([tokens])\n",
    "    logits = inference(x, pt.tensor([len(tokens)]))[0]\n",
    "    logits /= temperature\n",
    "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    token = pt.multinomial(probs, 1).numpy()[0]\n",
    "    tokens += [token]\n",
    "    sent = tokenizer.decode(tokens)\n",
    "    print(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=-1, top_p=0.9, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
