{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, ReformerModelWithLMHead, get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *\n",
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"\n",
    "device = pt.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ###   Options   ###\n",
    "\n",
    "model_name = \"ernst_one\"             # First, use a mean pooling of the claim and chained evidence\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set  (list sets)\n",
    "sample_test_n = 25                   # Number of randomly generated prompts for each sample when testing model\n",
    "log_period_batches = 25              # Batches per iteration\n",
    "learning_rate = 2e-5                 # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 8                 # Samples per batch\n",
    "N_train_batches = int(1e7 / bsz)     # Total number of batches to show model\n",
    "min_nw, max_nw = 0.17, 0.8           # Minimum and maximum fraction of list to keep when truncating\n",
    "max_listlen = 20                     # Maximum number of words in the list when creating a prompt (at least prior to * max_nw)\n",
    "lidstone_e = 0.0                     # Smoothing for possible words/subwords which are not in the missing list words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [(tokenizer.encode(prompt), \"types of\" in prompt) for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk, using nw_draw = 15. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p):  # prmt = prompt tokens, p = list phrases tokens list\n",
    "    tkzs, sent, tkix = [], [], 0\n",
    "    incl_words = np.random.choice(len(p), min(max_listlen, len(p)), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc = p[phz_i]\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent += phz_enc\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(round(tkix * min_nw), round(tkix * max_nw))\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if enc[:trunc_n] == partial_phr]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = 1 / norm, np.tile(lidstone_e / N_tokens, N_tokens)\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return np.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def gen_samples_uniform(xcp, xcs, xp, nw, verbose=False):  # Weight testing samples (word lists) uniformly\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        for m in range(nw):\n",
    "            prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "            cat_ix = np.random.randint(len(cp))\n",
    "            x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "            sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0:\n",
    "                sys_print(\"\\rDone: \" + str(j))\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        sqlens.append(sqlen)\n",
    "    if verbose: sys_print(\"\\rDone: \" + str(j) + \", finished!\\n\")\n",
    "    return xs, ys, sqlens\n",
    "def gen_samples(xcp, xcs, xp, n):  # Maximise training batch diversity by randomly sampling the word lists\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets = len(xcp)\n",
    "    for m in range(n):\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "        cat_ix = np.random.randint(len(cp))\n",
    "        x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "        sqlens.append(len(x_))\n",
    "    return xs, ys, sqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Done: 50, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_wordlists, N_test, replace=False)\n",
    "test_idx = np.array([3, 7])\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens = gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, sample_test_n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ld((test_xs, test_ys, test_sqlens), \"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_xs, test_ys, test_sqlens = load_ld(\"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens):\n",
    "    max_len = max(sqlens)\n",
    "    xs = pt.tensor(np.vstack([np.pad(x, (0, max_len - len(x)), constant_values=50256) for x in xs])).to(device)\n",
    "    ys = pt.tensor(np.vstack(ys)).to(device)#.cpu()\n",
    "    sqlens = pt.tensor(np.asarray(sqlens)).to(device)#.cpu()\n",
    "    return xs, ys, sqlens\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train\n",
    "    xs, ys, sqlens = gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz)\n",
    "    return adapt_form(xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2SeqClas\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2-large',\n",
    "    output_hidden_states=True, output_attentions=True, \n",
    "    cache_dir=\"models/pretrained/GPT2SeqClas\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "n_embd = model.config.n_embd\n",
    "model = nn.DataParallel(model, device_ids=list(range(pt.cuda.device_count()))).to(device) if dev != \"cpu\" else model.cpu()\n",
    "print(\"GPT2 loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = nn.Linear(n_embd, len(tokenizer), bias=False)#.cpu()\n",
    "nn.init.xavier_uniform_(llayer.weight)\n",
    "llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(device)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()#.cpu()\n",
    "bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(device)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(batch_size)\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)#.cpu()\n",
    "    sqlens_batch = sqlens_batch.to(device)#.cpu()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    outputs = model(x_batch.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens_batch - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1].to(device), 1, out_idx).squeeze(1)\n",
    "#     outs = pt.gather(outputs[2][-1].cpu(), 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    \n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float()).to(device)#.cpu()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.sum().backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens):\n",
    "    global model, llayer\n",
    "\n",
    "    x, sqlens = x.to(device), sqlens.to(device)#.cpu()\n",
    "    outputs = model(x.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1].to(device), 1, out_idx).squeeze(1)\n",
    "#     outs = pt.gather(outputs[2][-1].cpu(), 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    return logits\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float()).to(device)#.cpu()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_\n",
    "\n",
    "# top_next = [self.tokenizer.decode(i.item()).strip() for i in probs.topk(k)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        gc.collect()\n",
    "        if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "        b_loss, b_accuracy = train_step()\n",
    "        if verbose:\n",
    "            sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                      ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "        iter_loss.append(b_loss)\n",
    "        iter_accuracy.append(b_accuracy)\n",
    "        \n",
    "        if (batch_i - 1) % log_period_batches == 0:  # Test on test set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i])\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + 1)]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                print('\\n' + test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]))\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if test_a > best_acc:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "                         \"llayer\": llayer.state_dict(),\n",
    "#                          \"softrmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(\"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 : 0.0 0.0 loss: 0.6977143 0.6956725 Best: 0 inf idx: -1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAEvCAYAAAB8Ei19AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdf7SVZZn4//cVICyUQYQiE2dkDKYQDqgHtUWOh0oEGX/V+CNlxlolTaXjV5cscX3GH1lOlDPlx8JmkUP1aQatyY9ECYkWR81kRPrAxA8NRGdxpBkVATnqmTx4ff/Yz6HN4eDZcDZw9uH9Wmuv8zzXc9/Pcz9XrjbXfu5978hMJEmSJEnq7t5xsAcgSZIkSVIlLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1offBHsC+GDJkSB533HEHexj7xWuvvcbhhx9+sIfR7ZmnypinzpmjyvT0PC1fvvzlzHznwR5HLfO9WeapMuapc+aoMj09T3t6b67JAva4447jqaeeOtjD2C8aGxtpaGg42MPo9sxTZcxT58xRZXp6niLiPw/2GGqd780yT5UxT50zR5Xp6Xna03uzU4glSZIkSTXBAlaSJEmSVBMsYCVJ6iYiYnJEPBMR6yNi5h7aXBQRayJidUTMK4t/JSJWFa+Ly+KPRcSK4rUpIuYX8Rll8VURsSMijiqOPR8RvymO9cx5wZKkmlST34GVJKmniYhewGzgTKAJWBYRCzJzTVmbEcANwITM3BIR7yriU4GTgHFAX+CRiFiUma9m5ull/e8DfgyQmbcDtxfxc4BrMvOVsiFNzMyX998dS1Lte/PNN2lqaqKlpeWAX3vgwIGsXbv2gF+32vr168ewYcPo06dPRe0tYCVJ6h5OAdZn5gaAiLgXOA9YU9bmCmB2Zm4ByMwXi/go4JHMbAVaI2IlMBn4YVvHiBgAfAj4ZAfX/jhwT3VvR5J6vqamJgYMGMBxxx1HRBzQa2/fvp0BAwYc0GtWW2ayefNmmpqaGD58eEV9nEIsSVL3cAywsWy/qYiVGwmMjIjHI2JpREwu4iuBKRHRPyKGABOBY9v1vQD4eWa+Wh6MiP6Uit37ysIJLI6I5RExvUt3JUk9WEtLC4MHDz7gxWtPEREMHjx4r55g+wRWkqTuoaN//WS7/d7ACKABGAY8FhGjM3NxRIwHfgW8BDwBtLbr+3Hg7g6ucQ7weLvpwxMyc1MxRfmhiHg6Mx/dbcCl4nY6wNChQ2lsbOzkFmtTc3Nzj723ajJPlTFPnaulHA0cOJDm5uaDcu0dO3awffv2g3Ltamtpaan4f3MLWEmSuocmdn1qOgzY1EGbpZn5JvBcRDxDqaBdlpm3AbcBFIs7rWvrFBGDKU1RvqCD615Cu+nDmbmp+PtiRNxf9N2tgM3MOcAcgPr6+uypv0fY039rsVrMU2XMU+dqKUdr1649aNN4t2/fzo4dO5g3bx6f+9zn9rr/2Wefzbx58zjyyCMran/LLbdwxBFHcN111+31tTrTr18/TjzxxIraOoVYkqTuYRkwIiKGR8RhlArLBe3azKc0PZhiqvBIYENE9CqKVCKiDqgDFpf1uxD4aWbuMkcrIgYCZ1As7FTEDi++L0tEHA5MAlZV7S4lSVWzdetW7rrrrg6P7dix4237Lly4sOLitTuxgJUkqRsoFmC6EngQWAv8MDNXR8StEXFu0exBYHNErAGWADMyczPQh9J04jWUnohOK87XZrenrIULgMWZ+VpZbCjwy2IhqCeBBzLzZ9W7U0lStcycOZNnn32WcePGMWPGDBobG5k4cSKXXnopY8aMAeD888/n5JNP5oQTTmDOnDk7+x533HG8/PLLPP/887z//e/niiuu4IQTTmDSpEm88cYbb3vdFStWcNppp1FXV8cFF1zAli1bALjzzjsZNWoUdXV1XHLJJQA88sgjjBs3jnHjxnHiiSd2edqzU4glSeomMnMhsLBd7Kay7QSuLV7lbVoorUS8p/M27CH+XeC77WIbgLF7NXBJ0kExa9YsVq1axYoVK4DS9Osnn3ySVatW7VzVd+7cuRx11FG88cYbjB8/no997GMMHjx4l/OsW7eOe+65h29/+9tcdNFF3HfffUybNm2P1/3rv/5rvvGNb3DGGWdw00038YUvfIE77riDWbNm8dxzz9G3b1+2bt0KwD/8wz8we/ZsJkyYQHNzM/369evSPVvASpIkSVIXfeEnq1mz6dXOG+6FUe/5I24+54S96nPKKafs8pM0d955J/fffz8AGzduZN26dbsVsMOHD2fcuHEAnHzyyTz//PN7PP+2bdvYunUrZ5xxBgCXX345F154IQB1dXVcdtllnH/++Zx//vkATJgwgWuvvZbLLruMj370owwbNmyv7qc9pxBLkiRJUg9x+OGH79xubGzk4Ycf5oknnmDlypWceOKJHf5kTd++fXdu9+rVi9bW9gvZV+aBBx7g85//PMuXL+fkk0+mtbWVmTNncvfdd/PGG29w2mmn8fTTT+/Tudv4BFaSJEmSumhvn5RWw4ABA972O6Xbtm1j0KBB9O/fn6effpqlS5d2+ZoDBw5k0KBBPPbYY5x++ul8//vf54wzzuCtt95i48aNTJw4kQ9+8IPMmzeP5uZmNm/ezJgxYxgzZgxPPPEETz/9NO973/v2+foWsJIkSZJUgwYPHsyECRMYPXo0U6ZMYerUqbscnzx5Mv/0T/9EXV0df/Znf8Zpp51Wlet+73vf42/+5m94/fXX+dM//VO+853vsGPHDqZNm8a2bdvITK655hqOPPJIbrzxRpYsWUKvXr0YNWoUU6ZM6dK1LWAlSZIkqUbNmzdvl/3y39Dt27cvixYt6rBf2/dchwwZwqpVf/i1tD39zustt9yyc3vcuHEdPs395S9/uVvsG9/4xp6Gvk/8DqwkSZIkqSZYwEqSJEmSaoIFrCRJkiSpJljASpIkSZJqggWsJEmSJKkmWMBKkiRJkmqCBawkSZIk1aCtW7dy11137XP/O+64g9dff73DYw0NDTz11FP7fO79xQJWkiRJkmrQ/ixguysLWEmSJEmqQTNnzuTZZ59l3LhxzJgxA4Dbb7+d8ePHU1dXx8033wzAa6+9xtSpUxk7diyjR4/mBz/4AXfeeSebNm1i4sSJTJw48W2vc8899zBmzBhGjx7N9ddfD8COHTv4xCc+wejRoxkzZgxf//rXAbjzzjsZNWoUdXV1XHLJJVW/595VP6MkSZIkab+bNWsWq1atYsWKFQAsXryYdevW8eSTT5KZnHvuuTz66KO89NJLvOc97+GBBx4AYNu2bQwcOJCvfe1rLFmyhCFDhuzxGps2beL6669n+fLlDBo0iEmTJjF//nyOPfZYXnjhBVatWgWUnga3jem5556jb9++O2PVZAErSZIkSV21aCb812+qe853j4EpsypuvnjxYhYvXsyJJ54IQHNzM+vWreP000/nuuuu4/rrr+cv/uIvOP300ys+57Jly2hoaOCd73wnAJdddhmPPvooN954Ixs2bOCqq65i6tSpTJo0CYC6ujouu+wyzj//fM4///y9uNnKOIVYkiRJknqAzOSGG25gxYoVrFixgvXr1/OpT32KkSNHsnz5csaMGcMNN9zArbfeulfn7MigQYNYuXIlDQ0NzJ49m09/+tMAPPDAA3z+859n+fLlnHzyybS2tlbl3tr4BFaSJEmSumovnpRWy4ABA9i+ffvO/bPOOosbb7yRyy67jCOOOIIXXniBPn360NraylFHHcW0adM44ogj+O53v7tL/7ebQnzqqady9dVX8/LLLzNo0CDuuecerrrqKl5++WUOO+wwPvaxj3H88cfziU98grfeeouNGzcyceJEPvjBDzJv3jyam5s58sgjq3bPFrCSJEmSVIMGDx7MhAkTGD16NFOmTOH2229n7dq1fOADHwDgiCOO4F/+5V9Yv349M2bM4B3veAd9+vThW9/6FgDTp09nypQpHH300SxZsqTDaxx99NF8+ctfZuLEiWQmZ599Nueddx4rV67kk5/8JG+99RYAX/7yl9mxYwfTpk1j27ZtZCbXXHNNVYtXsICVJEmSpJo1b968Xfavvvpqrr766l1ixx9/PGedddZufa+66iquuuqqDs/b2Ni4c/vSSy/l0ksv3eX42LFj+fWvf71bv1/+8peVDn2fVOU7sBExOSKeiYj1ETGzg+N9I+IHxfF/j4jj2h3/44hojojrqjEeSZIkSVLP0+UCNiJ6AbOBKcAo4OMRMapds08BWzLzvcDXga+0O/51YFFXxyJJUi3r7APhos1FEbEmIlZHxLyy+FciYlXxurgs/lhErChemyJifhFviIhtZcdu2ptxSJJ0MFRjCvEpwPrM3AAQEfcC5wFrytqcB9xSbP8I+GZERGZmRJwPbABeq8JYJEmqSWUfCJ8JNAHLImJBZq4pazMCuAGYkJlbIuJdRXwqcBIwDugLPBIRizLz1cw8vaz/fcCPyy77WGb+xd6OQ5Kkg6UaU4iPATaW7TcVsQ7bZGYrsA0YHBGHA9cDX6jCOCRJqmU7PxDOzN8DbR8Il7sCmJ2ZWwAy88UiPgp4JDNbM/M1YCUwubxjRAwAPgTMr8I4JEmFPf3MjCqzt/mrxhPY6GgcFbb5AvD1zGyO6KhJ2QkipgPTAYYOHbrLl4p7kubm5h57b9Vknipjnjpnjipjng6Ijj4QPrVdm5EAEfE40Au4JTN/RqlgvTkivgb0Byay60wogAuAn2fmq2WxD0TESmATcF1mrq5wHJIkoF+/fmzevJnBgwfTWT2j3WUmmzdvpl+/fhX3qUYB2wQcW7Y/jNIbYUdtmiKiNzAQeIXSG+JfRsRXgSOBtyKiJTO/2f4imTkHmANQX1+fDQ0NVRh699PY2EhPvbdqMk+VMU+dM0eVMU8HRCUfCPcGRgANlN5vH4uI0Zm5OCLGA78CXgKeANr/cvzHgbvL9n8N/EnxIfLZlJ7MjqhwHKUB++GyypinypinztVSjiKCww8/nI0bN3beuMoys0cUzTt27OC1117jP//zPytqX40CdhkwIiKGAy8AlwCXtmuzALic0hvqXwK/yNKz4vLv5dwCNHdUvEqSdAio9APhpZn5JvBcRDxDqehclpm3AbcBFIs7rWvrFBGDKU0NvqAtVv4kNjMXRsRdETGkwnG09fPDZe1knipjnjpnjipzqOapy9+BLb7TeiXwILAW+GFmro6IWyPi3KLZP1P6zut64FrAFQ0lSdrVzg+EI+IwSh8IL2jXZj6l6cEUxeZIYENE9CqKVCKiDqgDFpf1uxD4aWa2tAUi4t1RfHQfEadQ+jfB5grHIUnSQVGNJ7Bk5kJgYbvYTWXbLZTePN/uHLdUYyySJNWizGyNiLYPhHsBc9s+EAaeyswFxbFJEbEG2AHMyMzNEdGP0nRigFeBacUHzG0uAWa1u+RfAp+NiFbgDeCSYnZUh+PYX/ctSdLeqEoBK0mSuq6CD4ST0kyma9u1aaG0EvGeztvQQeybQIdf2+loHJIkdQfV+BkdSZIkSZL2OwtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVBAtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVBAtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVBAtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkqZuIiMkR8UxErI+ImXtoc1FErImI1RExryz+lYhYVbwuLos/FhEritemiJhfxC+LiP8oXr+KiLFlfZ6PiN8UfZ7an/csSdLe6H2wByBJkiAiegGzgTOBJmBZRCzIzDVlbUYANwATMnNLRLyriE8FTgLGAX2BRyJiUWa+mpmnl/W/D/hxsfsccEZxninAHODUsiFNzMyX99f9SpK0L3wCK0lS93AKsD4zN2Tm74F7gfPatbkCmJ2ZWwAy88UiPgp4JDNbM/M1YCUwubxjRAwAPgTML/r+qu08wFJg2H64J0mSqsoCVpKk7uEYYGPZflMRKzcSGBkRj0fE0ohoK1JXAlMion9EDAEmAse263sB8PPMfLWDa38KWFS2n8DiiFgeEdP38X4kSao6pxBLktQ9RAexbLffGxgBNFB6YvpYRIzOzMURMR74FfAS8ATQ2q7vx4G7d7toxERKBewHy8ITMnNTMUX5oYh4OjMf7aDvdGA6wNChQ2lsbOz0JmtRc3Nzj723ajJPlTFPnTNHlTlU82QBK0lS99DErk9NhwGbOmizNDPfBJ6LiGcoFbTLMvM24DaAYnGndW2dImIwpSnKF5SfLCLqKBW1UzJzc1s8MzcVf1+MiPuLvrsVsJk5h9J3Z6mvr8+Ghoa9v+sa0NjYSE+9t2oyT5UxT50zR5U5VPPkFGJJkrqHZcCIiBgeEYcBlwAL2rWZT2l6MMVU4ZHAhojoVRSpbUVpHbC4rN+FwE8zs6UtEBF/DPxf4K8y87dl8cOL78sSEYcDk4BVVb1TSZL2kU9gJUnqBjKzNSKuBB4EegFzM3N1RNwKPJWZC4pjkyJiDbADmJGZmyOiH6XpxACvAtMys3wK8SXArHaXvAkYDNxV9GvNzHpgKHB/EesNzMvMn+2fu5Ykae9YwEqS1E1k5kJgYbvYTWXbCVxbvMrbtFBaiXhP523oIPZp4NMdxDcAY9vHJUnqDpxCLEmSJEmqCRawkiRJkqSaYAErSZIkSaoJFrCSJEmSpJpgAStJkiRJqgkWsJIkSZKkmmABK0mSJEmqCRawkiRJkqSaYAErSZIkSaoJVSlgI2JyRDwTEesjYmYHx/tGxA+K4/8eEccV8TMjYnlE/Kb4+6FqjEeSJEmS1PN0uYCNiF7AbGAKMAr4eESMatfsU8CWzHwv8HXgK0X8ZeCczBwDXA58v6vjkSRJkiT1TNV4AnsKsD4zN2Tm74F7gfPatTkP+F6x/SPgwxERmfn/MnNTEV8N9IuIvlUYkyRJkiSph+ldhXMcA2ws228CTt1Tm8xsjYhtwGBKT2DbfAz4f5n5Px1dJCKmA9MBhg4dSmNjYxWG3v00Nzf32HurJvNUGfPUOXNUGfMkSZK6g2oUsNFBLPemTUScQGla8aQ9XSQz5wBzAOrr67OhoWGvB1oLGhsb6an3Vk3mqTLmqXPmqDLmSZIkdQfVmELcBBxbtj8M2LSnNhHRGxgIvFLsDwPuB/46M5+twngkSZIkST1QNQrYZcCIiBgeEYcBlwAL2rVZQGmRJoC/BH6RmRkRRwIPADdk5uNVGIskSZIkqYfqcgGbma3AlcCDwFrgh5m5OiJujYhzi2b/DAyOiPXAtUDbT+1cCbwXuDEiVhSvd3V1TJIkSZKknqca34ElMxcCC9vFbirbbgEu7KDfl4AvVWMMkiRJkqSerRpTiCVJkiRJ2u8sYCVJkiRJNcECVpIkSZJUEyxgJUnqJiJickQ8ExHrI2LmHtpcFBFrImJ1RMwri38lIlYVr4vL4o+VLZS4KSLmF/GIiDuLa/1HRJxU1ufyiFhXvC5HkqRuoiqLOEmSpK6JiF7AbOBMSr+fviwiFmTmmrI2I4AbgAmZuaVt5f6ImAqcBIwD+gKPRMSizHw1M08v638f8ONidwowonidCnwLODUijgJuBuqBBJYX49iyH29fkqSK+ARWkqTu4RRgfWZuyMzfA/cC57VrcwUwu62YzMwXi/go4JHMbM3M14CVwOTyjhExAPgQML8InQf8nyxZChwZEUcDZwEPZeYrxXUean8uSZIOFgtYSZK6h2OAjWX7TUWs3EhgZEQ8HhFLI6KtsFwJTImI/hExBJgIHNuu7wXAzzPz1U6uV8k4JEk6KJxCLElS9xAdxLLdfm9KU34bgGHAYxExOjMXR8R44FfAS8ATQGu7vh8H7q7gepWMo3SCiOnAdIChQ4fS2NjYUbOa19zc3GPvrZrMU2XMU+fMUWUO1TxZwEqS1D00setT02HApg7aLM3MN4HnIuIZSgXtssy8DbgNoFjcaV1bp4gYTGmK8gUVXK+JUoFcHm/saMCZOQeYA1BfX58NDQ0dNat5jY2N9NR7qybzVBnz1DlzVJlDNU9OIZYkqXtYBoyIiOERcRhwCbCgXZv5lKYHU0wVHglsiIheRZFKRNQBdcDisn4XAj/NzJay2ALgr4vViE8DtmXm74AHgUkRMSgiBgGTipgkSQedT2AlSeoGMrM1Iq6kVCz2AuZm5uqIuBV4KjMX8Ificg2wA5iRmZsjoh+l6cQArwLTMrN8CvElwKx2l1wInA2sB14HPlmM45WI+CKlghrg1sx8ZT/csiRJe80CVpKkbiIzF1IqLMtjN5VtJ3Bt8Spv00JpJeI9nbehg1gCn99D+7nA3L0YuiRJB4RTiCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkqRuIiImR8QzEbE+Imbuoc1FEbEmIlZHxLyy+FciYlXxurgsHhFxW0T8NiLWRsTfFvEZEbGieK2KiB0RcVRx7PmI+E1x7Kn9fd+SJFWq98EegCRJgojoBcwGzgSagGURsSAz15S1GQHcAEzIzC0R8a4iPhU4CRgH9AUeiYhFmfkq8AngWOB9mflWW5/MvB24veh/DnBNZr5SNqSJmfnyfr1pSZL2kk9gJUnqHk4B1mfmhsz8PXAvcF67NlcAszNzC0BmvljERwGPZGZrZr4GrAQmF8c+C9yamW+161Pu48A9Vb0bSZL2AwtYSZK6h2OAjWX7TUWs3EhgZEQ8HhFLI6KtSF0JTImI/hExBJhI6akrwPHAxRHxVEQsKp7i7hQR/SkVu/eVhRNYHBHLI2J6Ve5OkqQqcAqxJEndQ3QQy3b7vYERQAMwDHgsIkZn5uKIGA/8CngJeAJoLfr0BVoysz4iPgrMBU4vO+c5wOPtpg9PyMxNxXTjhyLi6cx8dLcBl4rb6QBDhw6lsbFxr264VjQ3N/fYe6sm81QZ89Q5c1SZQzVPFrCSJHUPTfzhqSmUCtRNHbRZmplvAs9FxDOUCtplmXkbcBtAsbjTurI+bU9X7we+0+6cl9Bu+nBmbir+vhgR91Oa3rxbAZuZc4A5APX19dnQ0FDpvdaUxsZGeuq9VZN5qox56pw5qsyhmienEEuS1D0sA0ZExPCIOIxSYbmgXZv5lKYHU0wVHglsiIheETG4iNcBdcDisj4fKrbPAH7bdrKIGFjEflwWOzwiBrRtA5OAVVW8T0mS9llVCtjOlv2PiL4R8YPi+L9HxHFlx24o4s9ExFnVGI8kSbUmM1uBK4EHgbXADzNzdUTcGhHnFs0eBDZHxBpgCTAjMzcDfShNJ15D6YnotOJ8ALOAj0XEb4AvA58uu+wFwOJi4ac2Q4FfRsRK4Enggcz82f64Z0mS9laXpxBXsuw/8ClgS2a+NyIuAb5CaUGJUZQ+YT4BeA/wcESMzMwdXR2XJEm1JjMXAgvbxW4q207g2uJV3qaF0krEHZ1zKzB1D8e+C3y3XWwDMHavBy9J0gFQjSewlSz7fx7wvWL7R8CHIyKK+L2Z+T+Z+RywvjifJEmSJEm7qEYBW8my/zvbFFOatgGDK+wrSZIkSVJVViGuZNn/PbWppG/pBC7VrzLmqTLmqXPmqDLmSZIkdQfVKGArXfb/WKApInoDA4FXKuwLuFS/dmWeKmOeOmeOKmOeJElSd1CNKcSVLPu/ALi82P5L4BfFQhQLgEuKVYqHU/otuyerMCZJkiRJUg/T5SewmdkaEW3L/vcC5rYt+w88lZkLgH8Gvh8R6yk9eb2k6Ls6In4IrAFagc+7ArEkSZIkqSPVmEJcybL/LcCFe+h7G3BbNcYhSZIkSeq5qjGFWJIkSZKk/c4CVpIkSZJUEyxgJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSJEmSVBMsYCVJkiRJNcECVpIkSZJUEyxgJUmSJEk1wQJWkqRuIiImR8QzEbE+Imbuoc1FEbEmIlZHxLyy+FciYlXxurgsHhFxW0T8NiLWRsTfFvGGiNgWESuK1017Mw5Jkg6G3gd7AJIkCSKiFzAbOBNoApZFxILMXFPWZgRwAzAhM7dExLuK+FTgJGAc0Bd4JCIWZearwCeAY4H3ZeZbbX0Kj2XmX+ztOCRJOlh8AitJUvdwCrA+Mzdk5u+Be4Hz2rW5ApidmVsAMvPFIj4KeCQzWzPzNWAlMLk49lng1sx8q12froxDkqSDwiewkiR1D8cAG8v2m4BT27UZCRARjwO9gFsy82eUCtabI+JrQH9gItD2xPR44OKIuAB4CfjbzFxXHPtARKwENgHXZebqCsdBMY7pwHSAoUOH0tjYuLf3XBOam5t77L1Vk3mqjHnqnDmqzKGaJwtYSZK6h+gglu32ewMjgAZgGPBYRIzOzMURMR74FaUi9QmgtejTF2jJzPqI+CgwFzgd+DXwJ5nZHBFnA/OLc1cyjlIwcw4wB6C+vj4bGhoqvNXa0tjYSE+9t2oyT5UxT50zR5U5VPPkFGJJkrqHJkrfVW0zjNKT0fZtfpyZb2bmc8AzlIpOMvO2zByXmWdSKkLXlfW5r9i+H6gr2r+amc3F9kKgT0QMqXAckiQdFBawkiR1D8uAERExPCIOAy4BFrRrM5/S9GCKYnMksCEiekXE4CJeR6lIXVzW50PF9hnAb4t2746IKLZPofRvgs0VjkOSpIPCKcSSJHUDmdkaEVcCD1L6fuvczFwdEbcCT2XmguLYpIhYA+wAZmTm5hScBJwAABsISURBVIjoR2k6McCrwLTMbJtCPAv414i4BmgGPl3E/xL4bES0Am8Al2RmAh2OY/9nQJKkzlnASpLUTRRTeRe2i91Utp3AtcWrvE0LpZWIOzrnVmBqB/FvAt+sdBySJHUHTiGWJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVBAtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVBAtYSZIkSVJNsICVJEmSJNUEC1hJkiRJUk2wgJUkSZIk1QQLWEmSJElSTbCAlSRJkiTVhC4VsBFxVEQ8FBHrir+D9tDu8qLNuoi4vIj1j4gHIuLpiFgdEbO6MhZJkiRJUs/W1SewM4GfZ+YI4OfF/i4i4ijgZuBU4BTg5rJC9x8y833AicCEiJjSxfFIkiRJknqorhaw5wHfK7a/B5zfQZuzgIcy85XM3AI8BEzOzNczcwlAZv4e+DUwrIvjkSRJkiT1UL272H9oZv4OIDN/FxHv6qDNMcDGsv2mIrZTRBwJnAP87z1dKCKmA9MBhg4dSmNjY9dG3k01Nzf32HurJvNUGfPUOXNUGfMkSZK6g04L2Ih4GHh3B4f+V4XXiA5iWXb+3sA9wJ2ZuWFPJ8nMOcAcgPr6+mxoaKjw8rWlsbGRnnpv1WSeKmOeOmeOKmOeDoyImEzpw9xewN2Zudv6EBFxEXALpffSlZl5aRH/CjC1aPbFzPxBEQ/gS8CFwA7gW5l5Z0RcBlxftG8GPpuZK4s+zwPbi/atmVlf/buVJGnvdVrAZuZH9nQsIv47Io4unr4eDbzYQbMmoKFsfxjQWLY/B1iXmXdUNGJJknqgiOgFzAbOpPTeuSwiFmTmmrI2I4AbgAmZuaVt5lNETAVOAsYBfYFHImJRZr4KfAI4FnhfZr5VNlvqOeCM4jxTKL0fn1o2pImZ+fJ+vGVJkvZaV78DuwC4vNi+HPhxB20eBCZFxKBi8aZJRYyI+BIwEPj/ujgOSZJq3SnA+szcUKwNcS+ltSbKXQHMLtaUIDPbPjgeBTySma2Z+RqwEphcHPsscGtmvlXeJzN/1XYeYCmuQyFJqgFdLWBnAWdGxDpKnxjPAoiI+oi4GyAzXwG+CCwrXrdm5isRMYzSNORRwK8jYkVEfLqL45EkqVZ1umYEMBIYGRGPR8TSYsoxlArWKcVP1A0BJlJ66gpwPHBxRDwVEYuKp7jtfQpYVLafwOKIWF6sQSFJUrfQpUWcMnMz8OEO4k8Bny7bnwvMbdemiY6/HytJ0qHobdeMKPQGRlD6as4w4LGIGJ2ZiyNiPPAr4CXgCaC16NMXaMnM+oj4KKX349N3XjRiIqUC9oNl15mQmZuK6cYPRcTTmfnobgN2gUWVMU+VMU+dM0eVOVTz1NVViCVJUnU08YenplAqUDd10GZpZr4JPBcRz1AqaJdl5m3AbQARMQ9YV9bnvmL7fuA7bSeLiDrgbmBK8aE0AJm5qfj7YkTcT2l6824FrAssqpx5qox56pw5qsyhmqeuTiGWJEnVsQwYERHDI+Iw4BJKa02Um09pejDFVOGRwIaI6BURg4t4HVAHLC7r86Fi+wzgt0W7Pwb+L/BXmfnbtgtExOERMaBtm9LaFauqfK+SJO0Tn8BKktQNZGZrRFxJaaHDXsDczFwdEbcCT2XmAv6wMOIaSj9xMyMzN0dEP0rTiQFeBaZlZtsU4lnAv0bENZR+LqftKz43AYOBu4p+bT+XMxS4v4j1BuZl5s/29/1LklQJC1hJkrqJzFwILGwXu6lsO4Fri1d5mxZKiyJ2dM6t/OH3Ycvjn6ZsvYqy+AZg7D4MX5Kk/c4pxJIkSZKkmmABK0mSJEmqCRawkiRJkqSaYAErSZIkSaoJFrCSJEmSpJpgAStJkiRJqgkWsJIkSZKkmmABK0mSJEmqCRawkiRJkqSaYAErSZIkSaoJFrCSJEmSpJpgAStJkiRJqgkWsJIkSZKkmmABK0mSJEmqCRawkiRJkqSaYAErSZIkSaoJFrCSJEmSpJpgAStJkiRJqgkWsJIkSZKkmmABK0mSJEmqCRawkiRJkqSaYAErSVI3ERGTI+KZiFgfETP30OaiiFgTEasjYl5Z/CsRsap4XVwWj4i4LSJ+GxFrI+Jvy+J3Ftf6j4g4qazP5RGxrnhdvj/vWZKkvdH7YA9AkiRBRPQCZgNnAk3AsohYkJlrytqMAG4AJmTmloh4VxGfCpwEjAP6Ao9ExKLMfBX4BHAs8L7MfKutDzAFGFG8TgW+BZwaEUcBNwP1QALLi3Fs2b8ZkCSpcz6BlSSpezgFWJ+ZGzLz98C9wHnt2lwBzG4rJjPzxSI+CngkM1sz8zVgJTC5OPZZ4NbMfKtdn/OA/5MlS4EjI+Jo4Czgocx8pbjOQ2XnkiTpoLKAlSSpezgG2Fi231TEyo0ERkbE4xGxNCLaCsuVwJSI6B8RQ4CJlJ66AhwPXBwRT0XEouIp7ttdr5JxSJJ0UDiFWJKk7iE6iGW7/d6Upvw2AMOAxyJidGYujojxwK+Al4AngNaiT1+gJTPrI+KjwFzg9Le5XiXjKA04YjowHWDo0KE0Njbu8eZqWXNzc4+9t2oyT5UxT50zR5U5VPNkAStJUvfQxB+emkKpQN3UQZulmfkm8FxEPEOpoF2WmbcBtwEUizutK+tzX7F9P/CdTq7XRKlALo83djTgzJwDzAGor6/PhoaGjprVvMbGRnrqvVWTeaqMeeqcOarMoZonpxBLktQ9LANGRMTwiDgMuARY0K7NfErTgymmCo8ENkREr4gYXMTrgDpgcVmfDxXbZwC/LbYXAH9drEZ8GrAtM38HPAhMiohBETEImFTEJEk66HwCK0lSN5CZrRFxJaVisRcwNzNXR8StwFOZuYA/FJdrgB3AjMzcHBH9KE0nBngVmJaZbVOIZwH/GhHXAM3Ap4v4QuBsYD3wOvDJYhyvRMQXKRXUUFoA6pX9evOSJFXIAlaSpG4iMxdSKizLYzeVbSdwbfEqb9NCaSXijs65FZjaQTyBz++hz1xK35WVJKlbcQqxJEmSJKkmWMBKkiRJkmqCBawkSZIkqSZYwEqSJEmSaoIFrCRJkiSpJljASpIkSZJqQpcK2Ig4KiIeioh1xd9Be2h3edFmXURc3sHxBRGxqitjkSRJkiT1bF19AjsT+HlmjgB+XuzvIiKOAm4GTgVOAW4uL3Qj4qOUflhdkiRJkqQ96moBex7wvWL7e8D5HbQ5C3goM1/JzC3AQ8BkgIg4gtKPsX+pi+OQJEmSJPVwvbvYf2hm/g4gM38XEe/qoM0xwMay/aYiBvBF4B+B17s4Dkk6ZLz55ps0NTXR0tJywK45cOBA1q5de8Cut7/069ePYcOG0adPn4M9FEmStA86LWAj4mHg3R0c+l8VXiM6iGVEjAPem5nXRMRxFYxjOjAdYOjQoTQ2NlZ4+drS3NzcY++tmsxTZcxT52oxR0cccQRDhw7lmGOOIaKj/4utvh07dtCrV68Dcq39JTPZtm0bK1eupLnZb65IklSLOi1gM/MjezoWEf8dEUcXT1+PBl7soFkT0FC2PwxoBD4AnBwRzxfjeFdENGZmAx3IzDnAHID6+vpsaOiwWc1rbGykp95bNZmnypinztVijtauXcuwYcMOWPEKsH37dgYMGHDArre/DBgwgObmZurr6w/2UCRJ0j7o6ndgFwBtqwpfDvy4gzYPApMiYlCxeNMk4MHM/FZmviczjwM+CPx2T8WrJGlXB7J47UnMmyRJta2rBews4MyIWAecWewTEfURcTdAZr5C6buuy4rXrUVMklSDtm7dyl133bVPfc8++2y2bt1a5RFJkqRDRZcK2MzcnJkfzswRxd9XivhTmfnpsnZzM/O9xes7HZzn+cwc3ZWxSJIOjLcrYHfs2PG2fRcuXMiRRx65P4YlSZIOAV19AitJOsTMnDmTZ599lnHjxjFjxgwaGxuZOHEil156KWPGjAHg/PPP5+STT+aEE05gzpw5O/sed9xxvPzyyzz//PO8//3v54orruCEE05g0qRJvPHGG7td6yc/+QmnnnoqJ554Ih/5yEf47//+b6C0+NYnP/lJxowZQ11dHffddx8AP/vZzzjppJMYO3YsH/7whw9ANiRJ0oHU1Z/RkSQdRF/4yWrWbHq1qucc9Z4/4uZzTtjj8VmzZrFq1SpWrFgBlBbCevLJJ1m1ahXDhw8HYO7cuRx11FG88cYbjB8/no997GMMHjx4l/OsW7eOe+65h29/+9tcdNFF3HfffUybNm2XNh/84AdZunQpEcHdd9/NV7/6Vf7xH/+RL37xiwwcOJDf/OY3AGzZsoWXXnqJK664gkcffZThw4fzyit+W0WSpJ7GAlaS1GWnnHLKzuIV4M477+T+++8HYOPGjaxbt263Anb48OGMGzcOgJNPPpnnn39+t/M2NTVx8cUX87vf/Y7f//73O6/x8MMPc++99+5sN2jQIH7yk5/w53/+5zvbHHXUUVW9R0mSdPBZwEpSDXu7J6UH0uGHH75zu7GxkYcffpgnnniC/v3709DQQEtLy259+vbtu3O7V69eHU4hvuqqq7j22ms599xzaWxs5JZbbgFKv+nafkXhjmKSJKln8TuwkqS9MmDAALZv377H49u2bWPQoEH079+fp59+mqVLl+7ztbZt28YxxxwDwPe+972d8UmTJvHNb35z5/6WLVv4wAc+wCOPPMJzzz0H4BRiSZJ6IAtYSdJeGTx4MBMmTGD06NHMmDFjt+OTJ0+mtbWVuro6brzxRk477bR9vtYtt9zChRdeyOmnn86QIUN2xv/u7/6OLVu2MHr0aMaOHcuSJUt45zvfyZw5c/joRz/K2LFjufjii/f5upIkqXtyCrEkaa/Nmzdvl/2Ghoad23379mXRokUd9mv7nuuQIUNYtWrVzvh1113XYfvzzjuP8847b7f4EUccscsT2TZTpkxhypQpnQ2/24qIycD/BnoBd2fmrA7aXATcAiSwMjMvLeJfAaYWzb6YmT8o4t8FzgC2Fcc+kZkrImIGcFkR6w28H3hnZr4SEc8D24EdQGtm1lf5ViVJ2icWsJIkdQMR0QuYDZwJNAHLImJBZq4pazMCuAGYkJlbIuJdRXwqcBIwDugLPBIRizKzbYnqGZn5o/LrZebtwO1F/3OAa9p+z70wMTNf3h/3KknSvnIKsSRJ3cMpwPrM3JCZvwfuBdo/fr4CmJ2ZWwAy88UiPgp4JDNbM/M1YCUweS+u/XHgni6NXpKkA8ACVpKk7uEYYGPZflMRKzcSGBkRj0fE0mLKMZQK1ikR0T8ihgATgWPL+t0WEf8REV+PiL7lJ4yI/pSK3fvKwgksjojlETG967cmSVJ1OIVYkqTuoaPfAMp2+72BEUADMAx4LCJGZ+biiBgP/Ap4CXgCaC363AD8F3AYMAe4Hri17JznAI+3mz48ITM3FVOUH4qIpzPz0d0GXCpupwMMHTqUxsbGvbjd2tHc3Nxj762azFNlzFPnzFFlDtU8WcBKktQ9NLHrU9NhwKYO2izNzDeB5yLiGUoF7bLMvA24DSAi5gHrADLzd0Xf/4mI7wDtV8y6hHbThzNzU/H3xYi4n9L05t0K2MycQ6kopr6+PssX8+pJGhsb6an3Vk3mqTLmqXPmqDKHap6cQixJUvewDBgREcMj4jBKheWCdm3mU5oeTDFVeCSwISJ6RcTgIl4H1AGLi/2ji78BnA/sXP45IgZSWqH4x2WxwyNiQNs2MKm8jyRJB5MFrCRpr2zdupW77rprn/vfcccdvP7661UcUc+Qma3AlcCDwFrgh5m5OiJujYhzi2YPApsjYg2whNLqwpuBPpSmE6+h9ER0WnE+gH+NiN8AvwGGAF8qu+wFwOJi4ac2Q4FfRsRK4Enggcz82f64Z0mS9pZTiCVJe6WtgP3c5z63T/3vuOMOpk2bRv/+/as8stqXmQuBhe1iN5VtJ3Bt8Spv00JpJeKOzvmht7ned4HvtottAMbu3cglSTowfAIrSdorM2fO5Nlnn2XcuHHMmDEDgNtvv53x48dTV1fHzTffDMBrr73G1KlTGTt2LKNHj+YHP/gBd955J5s2bWLixIlMnDhxt3PfeuutjB8/ntGjRzN9+nRK9RqsX7+ej3zkI4wdO5aTTjqJZ599FoCvfvWrjBkzhrFjxzJz5swDlAFJknSw+ARWkmrZopnwX7+p7jnfPQamzNrj4VmzZrFq1SpWrFgBwOLFi1m3bh1PPvkkmcm5557Lo48+yksvvcR73vMeHnjgAQC2bdvGwIED+drXvsaSJUsYMmTIbue+8soruemm0gPHv/qrv+KnP/0p55xzDpdddhkzZ87kggsuoKWlhbfeeotFixYxf/58/v3f/53+/fvzyiuv7HY+SZLUs/gEVpLUJYsXL2bx4sWceOKJnHTSSTz99NOsW7eOMWPG8PDDD3P99dfz2GOPMXDgwE7PtWTJEk499VTGjBnDL37xC1avXs327dt54YUXuOCCCwDo168f/fv35+GHH+aTn/zkzqnIRx111H69T0mSdPD5BFaSatnbPCk9UDKTG264gc985jO7HVu+fDkLFy7khhtuYNKkSTufrnakpaWFz33uczz11FMce+yx3HLLLbS0tOycRtzRdUsL60qSpEOFT2AlSXtlwIABbN++fef+WWedxdy5c2lubgbghRde4MUXX2TTpk3079+fadOmcd111/HrX/+6w/5tWlpaABgyZAjNzc386Ec/AuCP/uiPGDZsGPPnzwfgf/7nf3j99deZNGkSc+fO3bmisVOIJUnq+XwCK0naK4MHD2bChAmMHj2aKVOmcPvtt7N27Vo+8IEPAHDEEUfwL//yL6xfv54ZM2bwjne8gz59+vCtb30LgOnTpzNlyhSOPvpolixZsvO8Rx55JFdccQVjxozhuOOOY/z48TuPff/73+czn/kMN910E3369OHf/u3fmDx5MitWrKC+vp7DDjuMs88+m7//+78/sMmQJEkHlAWsJGmvzZs3b5f9q6++mquvvnqX2PHHH89ZZ521W9+rrrqKq666qsPzfulLX+JLX/rSbvERI0bwi1/8Yrf4zJkzXX1YkqRDiFOIJUmSJEk1wQJWkiRJklQTLGAlSZIkSTXBAlaSatCeflpGb8+8SZJU2yxgJanG9OvXj82bN1uM7aXMZPPmzfTr1+9gD0WSJO0jVyGWpBozbNgwmpqaeOmllw7YNVtaWnpE4devXz+GDRt2sIchSZL2kQWsJNWYPn36MHz48AN6zcbGRk488cQDek1JkqT2nEIsSZIkSaoJFrCSJEmSpJpgAStJkiRJqglRi6tYRsRLwH8e7HHsJ0OAlw/2IGqAeaqMeeqcOapMT8/Tn2TmOw/2IGqZ783CPFXKPHXOHFWmp+epw/fmmixge7KIeCoz6w/2OLo781QZ89Q5c1QZ86RDmf/9V8Y8VcY8dc4cVeZQzZNTiCVJkiRJNcECVpIkSZJUEyxgu585B3sANcI8VcY8dc4cVcY86VDmf/+VMU+VMU+dM0eVOSTz5HdgJUmSJEk1wSewkiRJkqSaYAF7EETEURHxUESsK/4O2kO7/7+d+wmxqozDOP59UFQsSA2mtClGyY21KIoiKLB/owWVlAtXDUmrVhVCyrTIkkghkgiKaOOmlIpIcCFmCdGiojRSapzRgialFkZgkhH9Wpx34DCcuR7vnXPOvdfnAy/33HPee/i9v7n6zJm5c0bSnHFJIwXH90o6Wn3FzeikT5IWSton6UdJxyS9Um/11ZK0VtKYpAlJmwuOz5e0Jx3/UtJQ7tiWtH9M0po6665bu32SdL+kbyR9nx7vqbv2OnXyfkrHr5N0VtKmumo2m23O5nKczTNzNpfjbC7H2dxCRHjUPIAdwOa0vRnYXjBnCXAyPS5O24tzxx8F3gWONr2ebuwTsBC4O82ZB3wOPND0mmapL3OAE8CKtLbvgFXT5jwFvJW2NwB70vaqNH8+sDydZ07Ta+rCPt0MLEvbNwK/Nr2ebuxT7viHwPvApqbX4+HR7nA2V98nZ7Oz2dlcfZ9yx/s2m/0b2GY8AuxK27uAdQVz1gAHIuJMRPwBHADWAki6HHgW2FZDrU1qu08RcS4iPgOIiH+Ab4HBGmquw23AREScTGvbTdarvHzvPgDulaS0f3dEnI+In4CJdL5+1HafIuJwRJxK+48BCyTNr6Xq+nXyfkLSOrJvTo/VVK9ZVZzN5Tibizmby3E2l+NsbsEXsM24KiJOA6THgYI51wC/5J5Ppn0ALwGvAueqLLILdNonACQtAh4CDlZUZ90uuOb8nIj4F/gTuLLka/tFJ33Keww4HBHnK6qzaW33SdJlwHPA1hrqNKuas7kcZ3MxZ3M5zuZynM0tzG26gH4l6RPg6oJDo2VPUbAvJN0EXB8Rz0z/rHsvqqpPufPPBd4DXo+IkxdfYVdqueYLzCnz2n7RSZ+yg9INwHZgeBbr6jad9Gkr8FpEnE0/9DXras7mcpzNbXE2l+NsLsfZ3IIvYCsSEffNdEzSb5KWRsRpSUuB3wumTQKrc88HgUPAHcAtkn4m+/oNSDoUEavpQRX2acrbwHhE7JyFcrvFJHBt7vkgcGqGOZPpG4UrgDMlX9svOukTkgaBj4DHI+JE9eU2ppM+3Q6sl7QDWAT8J+nviHij+rLNLp6zuRxnc1uczeU4m8txNrfgjxA3Yy8wdefCEeDjgjn7gWFJi9Md/oaB/RHxZkQsi4gh4E7geK8GZAlt9wlA0jayf8xP11Brnb4GVkpaLmke2R/u7502J9+79cCnERFp/4Z057rlwErgq5rqrlvbfUofbdsHbImIL2qruBlt9yki7oqIofT/0U7g5X4KSLvkOJvLcTYXczaX42wux9ncSlV3h/KYeZB9jv8gMJ4el6T9twLv5OZtJPtD/gngiYLzDNHfdzpsu09kP6kK4AfgSBpPNr2mWezNg8BxsjvUjaZ9LwIPp+0FZHeemyALwRW5146m143RJ3d/nO0+Ac8Df+XeO0eAgabX0219mnaOF+jDOx16XDrD2Vx9n5zNzuZO+uRsdjZPDaXFmZmZmZmZmXU1f4TYzMzMzMzMeoIvYM3MzMzMzKwn+ALWzMzMzMzMeoIvYM3MzMzMzKwn+ALWzMzMzMzMeoIvYM3MzMzMzKwn+ALWzMzMzMzMeoIvYM3MzMzMzKwn/A9EayWxNRtg7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.78 GiB total capacity; 13.90 GiB already allocated; 21.75 MiB free; 14.23 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miterate_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-30c03d2258f6>\u001b[0m in \u001b[0;36miterate_training\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
      "\u001b[0;32m<ipython-input-13-bace995c9c5a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.78 GiB total capacity; 13.90 GiB already allocated; 21.75 MiB free; 14.23 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load a model\n",
    "# gc.collect()\n",
    "# checkpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# llayer.load_state_dict(checkpoint['llayer'])\n",
    "# bcewl_loss.load_state_dict(checkpoint['bcewl_loss'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_next_token(sent, top_k=-1, top_p=0.9, temperature=1.0):\n",
    "    global model\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    x = pt.tensor([tokens])\n",
    "    if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "    logits = inference(x, pt.tensor([len(tokens)]))[0]\n",
    "    logits /= temperature\n",
    "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    token = pt.multinomial(probs, 1).numpy()[0]\n",
    "    tokens += [token]\n",
    "    sent = tokenizer.decode(tokens)\n",
    "    print(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=-1, top_p=0.9, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
