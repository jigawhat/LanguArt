{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, GPT2LMHeadModel, ReformerModelWithLMHead, \\\n",
    "                         get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *\n",
    "dev = \"cuda\" if pt.cuda.is_available() else \"cpu\"\n",
    "d = device = pt.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ###   Options   ###\n",
    "\n",
    "model_name = \"ernst_one\"             # First, use a mean pooling of the claim and chained evidence\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set  (list sets)\n",
    "sample_test_n = 25                   # Number of randomly generated prompts for each sample when testing model\n",
    "log_period_batches = 25              # Batches per iteration\n",
    "learning_rate = 2e-5                 # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 8                 # Samples per batch\n",
    "N_train_batches = int(1e7 / bsz)     # Total number of batches to show model\n",
    "min_nw, max_nw = 0.17, 0.8           # Minimum and maximum fraction of list to keep when truncating\n",
    "max_listlen = 20                     # Maximum number of words in the list when creating a prompt (at least prior to * max_nw)\n",
    "lidstone_e = 0.0                     # Smoothing for possible words/subwords which are not in the missing list words set\n",
    "max_len = 96                         # Manually specified in order to match a power of 2 (todo: find opt 64 < x < 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[tokenizer.encode(prompt), \"types of\" in prompt] for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [[pt.tensor(prmt, device=d), pt.tensor(typesof, device=d)] for (prmt, typesof) in lprompts_encoded]\n",
    "cats_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_e]\n",
    "cats_sing_e = [[pt.tensor(c, device=d) for c in cs] for cs in cats_sing_e]\n",
    "phrases_e = [[pt.tensor(p, device=d) for p in ps] for ps in phrases_e]\n",
    "N_tokens = pt.tensor(N_tokens, device=d)\n",
    "max_len = pt.tensor(max_len, device=d)\n",
    "bsz = pt.tensor(batch_size, device=d)\n",
    "y_zero = (lidstone_e / N_tokens).repeat(N_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk, using nw_draw = 15. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p):  # prmt = prompt tokens, p = list phrases tokens list\n",
    "    tkzs, sent, tkix = [], [], 0\n",
    "#     incl_words = pt.randperm(len(p))[:min(max_listlen, len(p))]\n",
    "    incl_words = np.random.choice(len(p), min(max_listlen, len(p)), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc = p[phz_i]\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent.append(phz_enc)\n",
    "    sent = pt.hstack(sent)\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(round(tkix * min_nw), round(tkix * max_nw))\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if len(enc) >= trunc_n and all(enc[:trunc_n] == partial_phr)]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = pt.tensor(1 / norm, device=d), y_zero.clone()\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return pt.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def gen_samples_uniform(xcp, xcs, xp, nw, verbose=False):  # Weight testing samples (word lists) uniformly\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        for m in range(nw):\n",
    "            prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "            cat_ix = np.random.randint(len(cp))\n",
    "            x_, y_ = gen_truncated_list(pt.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "            sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0:\n",
    "                sys_print(\"\\rDone: \" + str(j))\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        sqlens.append(sqlen)\n",
    "    if verbose: sys_print(\"\\rDone: \" + str(j) + \", finished!\\n\")\n",
    "    return xs, ys, sqlens\n",
    "def gen_samples(xcp, xcs, xp, n):  # Maximise training batch diversity by randomly sampling the word lists\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets = len(xcp)\n",
    "    for m in range(n):\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "        cat_ix = np.random.randint(len(cp))\n",
    "        x_, y_ = gen_truncated_list(pt.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "        sqlens.append(len(x_))\n",
    "    return xs, ys, sqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Done: 50, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_wordlists, N_test, replace=False)\n",
    "test_idx = np.array([3, 7])\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens = gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, sample_test_n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ld((test_xs, test_ys, test_sqlens), \"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_xs, test_ys, test_sqlens = load_ld(\"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens):\n",
    "    xs = pt.vstack([F.pad(x, (0, max_len - len(x)), mode='constant', value=pad_token) for x in xs])\n",
    "    return xs, pt.vstack(ys), pt.tensor(sqlens, device=d)\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train\n",
    "    return adapt_form(*gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "GPT2 parameters device: cuda:0\n",
      "Pretrained parameters loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2LMHead\")\n",
    "# model_ = GPT2ForSequenceClassification.from_pretrained('gpt2-large',\n",
    "model_ = GPT2LMHeadModel.from_pretrained('gpt2-large',\n",
    "    output_hidden_states=True, output_attentions=True, \n",
    "    cache_dir=\"models/pretrained/GPT2LMHead\")\n",
    "# model_.parallelize()\n",
    "model_ = model_.to(d)\n",
    "print(\"GPT2 parameters device:\", model_.device)\n",
    "model_.resize_token_embeddings(N_tokens)\n",
    "pad_token = model_.config.pad_token_id = model_.config.eos_token_id\n",
    "pad_token = pt.tensor(pad_token, device=d)\n",
    "n_embd = pt.tensor(model_.config.n_embd, device=d)\n",
    "# model = nn.parallel.DistributedDataParallel(\n",
    "model = nn.DataParallel(\n",
    "    model_, device_ids=list(range(pt.cuda.device_count()))) if dev != \"cpu\" else model\n",
    "print(\"Pretrained parameters loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = nn.Linear(n_embd, N_tokens, bias=False).to(d)#.cpu()\n",
    "nn.init.xavier_uniform_(llayer.weight)\n",
    "llayer = nn.DataParallel(llayer, device_ids=list(range(pt.cuda.device_count())))\n",
    "# llayer = nn.parallel.DistributedDataParallel(llayer, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss().to(d)#.cpu()\n",
    "bcewl_loss = nn.DataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# bcewl_loss = nn.parallel.DistributedDataParallel(bcewl_loss, device_ids=list(range(pt.cuda.device_count()))).to(d)\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def sequence_mask(lengths, maxlen=None, dtype=pt.int):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    row_vector = pt.arange(0, maxlen, 1, device=d)\n",
    "    matrix = pt.unsqueeze(lengths, dim=-1)\n",
    "    mask = row_vector < matrix\n",
    "\n",
    "    mask = mask.type(dtype)\n",
    "    return mask\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, scheduler, bsz\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(bsz)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    mask = sequence_mask(sqlens_batch, max_len)\n",
    "    outputs = model(x_batch.long(), attention_mask=mask)\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens_batch - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1].to(d), 1, out_idx).squeeze(1)\n",
    "#     outs = pt.gather(outputs[2][-1].cpu(), 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    \n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())#.to(d)#.cpu()\n",
    "    loss = loss.mean()\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens):\n",
    "    global model, llayer\n",
    "\n",
    "    x, sqlens = x.to(d), sqlens.to(d)#.cpu()\n",
    "    mask = sequence_mask(sqlens, max_len)\n",
    "    outputs = model(x.long(), attention_mask=mask)\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens - 1, 1).repeat((1, n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1], 1, out_idx).squeeze(1)\n",
    "#     outs = pt.gather(outputs[2][-1].cpu(), 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    return logits\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "\n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())#.to(d)#.cpu()\n",
    "        loss = loss.mean()\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "        loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    return loss_, correct_\n",
    "\n",
    "# top_next = [self.tokenizer.decode(i.item()).strip() for i in probs.topk(k)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        gc.collect()\n",
    "        if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "        b_loss, b_accuracy = train_step()\n",
    "        if verbose:\n",
    "            sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                      ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete.                  ')\n",
    "        iter_loss.append(b_loss)\n",
    "        iter_accuracy.append(b_accuracy)\n",
    "        \n",
    "        if (batch_i - 1) % log_period_batches == 0:  # Test on test set\n",
    "            model.eval()\n",
    "            loss, accuracy = [], []\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i])\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + 1)]\n",
    "                if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                print('\\n' + test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]))\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if test_a > best_acc:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "                         \"llayer\": llayer.state_dict(),\n",
    "#                          \"softrmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(\"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "            model.train()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 : 0.0 0.0 loss: 0.6981167 0.6966449 Best: 0 inf idx: -1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 13.95 GiB already allocated; 3.75 MiB free; 14.25 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miterate_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-30c03d2258f6>\u001b[0m in \u001b[0;36miterate_training\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
      "\u001b[0;32m<ipython-input-14-67703abebedb>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 13.95 GiB already allocated; 3.75 MiB free; 14.25 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load a model\n",
    "# gc.collect()\n",
    "# checkpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# llayer.load_state_dict(checkpoint['llayer'])\n",
    "# bcewl_loss.load_state_dict(checkpoint['bcewl_loss'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_next_token(sent, top_k=-1, top_p=0.9, temperature=1.0):\n",
    "    global model\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    x = pt.tensor([tokens], device=d)\n",
    "    if dev != \"cpu\": pt.cuda.empty_cache()\n",
    "    logits = inference(x, pt.tensor([len(tokens)]), device=d)[0]\n",
    "    logits /= temperature\n",
    "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    token = pt.multinomial(probs, 1).numpy()[0]\n",
    "    tokens += [token]\n",
    "    sent = tokenizer.decode(tokens)\n",
    "    print(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=-1, top_p=0.9, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
