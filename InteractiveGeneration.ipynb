{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from transformers import GPT2ForSequenceClassification, ReformerModelWithLMHead, get_linear_schedule_with_warmup\n",
    "from pytorch_transformers import GPT2Tokenizer\n",
    "from Learning import *\n",
    "device = pt.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0121 08:06:59.558473 42632 tokenization_utils.py:384] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json from cache at C:\\Users\\alfew\\.cache\\torch\\pytorch_transformers\\69f8d734111f39eaa51a85907bfdc81a7ef42242d638ffab6f77df305402b2b2.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0121 08:06:59.560468 42632 tokenization_utils.py:384] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt from cache at C:\\Users\\alfew\\.cache\\torch\\pytorch_transformers\\38d28acc17953e356348dca948e152c653c0ccf5058a552eea30168e27f02046.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats, cats_sing, phrases = Listset().load()  # Import word lists dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('???', [28358, 34913]),\n",
       " ('���', [47490, 48585]),\n",
       " ('??', [3548, 19153]),\n",
       " ('の�', [15474, 17683, 27032, 33426, 49149]),\n",
       " (' 裏�', [20174, 34504]),\n",
       " ('._', [13557, 47540]),\n",
       " ('.', [13, 764]),\n",
       " ('...', [986, 2644]),\n",
       " ('.\"', [526, 22135]),\n",
       " ('�',\n",
       "  [94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   447,\n",
       "   1209,\n",
       "   1792,\n",
       "   2515,\n",
       "   4210,\n",
       "   5008,\n",
       "   5099,\n",
       "   6552,\n",
       "   8582,\n",
       "   10310,\n",
       "   11737,\n",
       "   11976,\n",
       "   12859,\n",
       "   13783,\n",
       "   15926,\n",
       "   17312,\n",
       "   17358,\n",
       "   17739,\n",
       "   17992,\n",
       "   18004,\n",
       "   18796,\n",
       "   19021,\n",
       "   19469,\n",
       "   19526,\n",
       "   19567,\n",
       "   20015,\n",
       "   20046,\n",
       "   20998,\n",
       "   22522,\n",
       "   22755,\n",
       "   22880,\n",
       "   22887,\n",
       "   23626,\n",
       "   23877,\n",
       "   24231,\n",
       "   24583,\n",
       "   24861,\n",
       "   25001,\n",
       "   26193,\n",
       "   26292,\n",
       "   26344,\n",
       "   26486,\n",
       "   27670,\n",
       "   27764,\n",
       "   27950,\n",
       "   28156,\n",
       "   28839,\n",
       "   28938,\n",
       "   29705,\n",
       "   29773,\n",
       "   29785,\n",
       "   29826,\n",
       "   30266,\n",
       "   30298,\n",
       "   30585,\n",
       "   31479,\n",
       "   31965,\n",
       "   32003,\n",
       "   32368,\n",
       "   32391,\n",
       "   32432,\n",
       "   32518,\n",
       "   32573,\n",
       "   32849,\n",
       "   33176,\n",
       "   33232,\n",
       "   33566,\n",
       "   33699,\n",
       "   33768,\n",
       "   34402,\n",
       "   34460,\n",
       "   34650,\n",
       "   34932,\n",
       "   35266,\n",
       "   35705,\n",
       "   35707,\n",
       "   35975,\n",
       "   36181,\n",
       "   36365,\n",
       "   36685,\n",
       "   37239,\n",
       "   37345,\n",
       "   37605,\n",
       "   37772,\n",
       "   37863,\n",
       "   38184,\n",
       "   38461,\n",
       "   39355,\n",
       "   39611,\n",
       "   40367,\n",
       "   41340,\n",
       "   41585,\n",
       "   41753,\n",
       "   41840,\n",
       "   43102,\n",
       "   43297,\n",
       "   43380,\n",
       "   43636,\n",
       "   43718,\n",
       "   43889,\n",
       "   43897,\n",
       "   44165,\n",
       "   44293,\n",
       "   45250,\n",
       "   45379,\n",
       "   45495,\n",
       "   45617,\n",
       "   45739,\n",
       "   45784,\n",
       "   45911,\n",
       "   46237,\n",
       "   46256,\n",
       "   46349,\n",
       "   46479,\n",
       "   46695,\n",
       "   46763,\n",
       "   47078,\n",
       "   47249,\n",
       "   47728,\n",
       "   47797,\n",
       "   47947,\n",
       "   47991,\n",
       "   48071,\n",
       "   48958,\n",
       "   49035,\n",
       "   49426,\n",
       "   49694]),\n",
       " ('.)', [2014, 46328]),\n",
       " ('?)', [10091, 41349]),\n",
       " ('��',\n",
       "  [4204,\n",
       "   6353,\n",
       "   6408,\n",
       "   7134,\n",
       "   8008,\n",
       "   8955,\n",
       "   10253,\n",
       "   11805,\n",
       "   13945,\n",
       "   18433,\n",
       "   21253,\n",
       "   23329,\n",
       "   23596,\n",
       "   25081,\n",
       "   26534,\n",
       "   31204,\n",
       "   35050,\n",
       "   35069,\n",
       "   36596,\n",
       "   39333,\n",
       "   41365,\n",
       "   41678,\n",
       "   42062,\n",
       "   43518,\n",
       "   43769,\n",
       "   45433,\n",
       "   45539,\n",
       "   45865,\n",
       "   46788,\n",
       "   48953,\n",
       "   50159]),\n",
       " ('....', [1106, 19424]),\n",
       " ('!', [0, 5145]),\n",
       " (' �',\n",
       "  [564,\n",
       "   1587,\n",
       "   2343,\n",
       "   5525,\n",
       "   6184,\n",
       "   7377,\n",
       "   10263,\n",
       "   10545,\n",
       "   11019,\n",
       "   12466,\n",
       "   12520,\n",
       "   13305,\n",
       "   13328,\n",
       "   14360,\n",
       "   14519,\n",
       "   14524,\n",
       "   15139,\n",
       "   16268,\n",
       "   17433,\n",
       "   17550,\n",
       "   17804,\n",
       "   18074,\n",
       "   18872,\n",
       "   18923,\n",
       "   20543,\n",
       "   20724,\n",
       "   23294,\n",
       "   23821,\n",
       "   24966,\n",
       "   25370,\n",
       "   27332,\n",
       "   28053,\n",
       "   28225,\n",
       "   30325,\n",
       "   31619,\n",
       "   34719,\n",
       "   34754,\n",
       "   36469,\n",
       "   42164,\n",
       "   42314,\n",
       "   42527,\n",
       "   43074,\n",
       "   50169]),\n",
       " ('./', [19571, 24457]),\n",
       " ('......', [16317, 47082]),\n",
       " ('龍�', [19049, 39820]),\n",
       " (',\"', [553, 42911]),\n",
       " ('?', [30, 5633]),\n",
       " ('...\"', [9313, 35713]),\n",
       " ('................', [4181, 44713]),\n",
       " ('!!', [3228, 37867]),\n",
       " ('........', [2109, 20004]),\n",
       " ('..', [492, 11485]),\n",
       " (',', [11, 837])]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we do not use any of the below characters (commas and fullstops always preceded by a word), we do not have to worry\n",
    "# about dividing our token-specific probability among the various possible token encodings\n",
    "tkn_map = {i: tokenizer.decode(i) for i in range(len(tokenizer))}\n",
    "tkn_invmap = {w: [i for i in tkn_map if tkn_map[i] == w] for w in set(tkn_map.values())}\n",
    "[(k, tkn_invmap[k]) for k in tkn_invmap if len(tkn_invmap[k]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([317, 1351, 286, 2835, 15921, 25, 22514, 11, 48389, 11, 279, 4127, 11],\n",
       " [317, 1351, 286, 2835, 15921, 25, 22514, 11],\n",
       " [48389, 11, 279, 4127, 11])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"A list of round fruits: apples, oranges, pears,\"), \\\n",
    "    tokenizer.encode(\"A list of round fruits: apples,\"), \\\n",
    "    tokenizer.encode(\"oranges, pears,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ###   Options   ###\n",
    "\n",
    "model_name = \"ernst_one\"             # First, use a mean pooling of the claim and chained evidence\n",
    "test_set_frac = 0.25                 # Fraction of samples to keep as separate test set  (list sets)\n",
    "sample_test_n = 25                   # Number of randomly generated prompts for each sample when testing model\n",
    "log_period_batches = 25              # Batches per iteration\n",
    "learning_rate = 5e-5                 # Adam learning rate (default is 5e-5, sentiment classification example had 2e-5)\n",
    "adam_epsilon = 1e-8                  # Adam epsilon (default is 1e-8)\n",
    "n_sched_warmup = 0                   # Linear scheduler for optimizer number of warmup steps\n",
    "batch_size = bsz = 8                 # Samples per batch\n",
    "N_train_batches = int(1e7 / bsz)     # Total number of batches to show model\n",
    "min_nw, max_nw = 0.17, 0.8           # Minimum and maximum fraction of list to keep when truncating\n",
    "max_listlen = 20                     # Maximum number of words in the list when creating a prompt (at least prior to * max_nw)\n",
    "lidstone_e = 0.0                     # Smoothing for possible words/subwords which are not in the missing list words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprompts_encoded = [(tokenizer.encode(prompt), \"types of\" in prompt) for prompt in lprompts]\n",
    "cats_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats]\n",
    "cats_sing_e = [[tokenizer.encode(c + ': ') for c in cs] for cs in cats_sing]\n",
    "phrases_e = [[tokenizer.encode(p + ', ') for p in ps] for ps in phrases]\n",
    "N_tokens = len(tokenizer)\n",
    "N_wordlists = len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fixed test set and save to disk, using nw_draw = 15. This function defines the next list token prediction problem\n",
    "def gen_truncated_list(prmt, p):  # prmt = prompt tokens, p = list phrases tokens list\n",
    "    tkzs, sent, tkix = [], [], 0\n",
    "    incl_words = np.random.choice(len(p), min(max_listlen, len(p)), replace=False)\n",
    "    for phz_i in incl_words:\n",
    "        phz_enc = p[phz_i]\n",
    "        tkzs.append((tkix, phz_enc))\n",
    "        tkix += len(phz_enc)\n",
    "        sent += phz_enc\n",
    "    missing_w = [p[i] for i in range(len(p)) if i not in incl_words]\n",
    "    trunc_ix = np.random.randint(round(tkix * min_nw), round(tkix * max_nw))\n",
    "    trunc_n = min([(trunc_ix - ix) for (ix, enc) in tkzs if ix <= trunc_ix])  # N. end phrase tokens\n",
    "    missing_w += [enc for (ix, enc) in tkzs if ix >= (trunc_ix - trunc_n)]\n",
    "    missing_matches = missing_w\n",
    "    if trunc_n > 0:\n",
    "        phr_start = trunc_ix - trunc_n\n",
    "        partial_phr = sent[phr_start:trunc_ix]\n",
    "        missing_matches = [enc for enc in missing_w if enc[:trunc_n] == partial_phr]\n",
    "    next_tokens = [enc[trunc_n] for enc in missing_matches]\n",
    "    norm = len(next_tokens) * (1.0 + lidstone_e)\n",
    "    tunit, y_ = 1 / norm, np.tile(lidstone_e / N_tokens, N_tokens)\n",
    "    for token in next_tokens: y_[token] += tunit\n",
    "    return np.hstack([prmt, sent[:trunc_ix]]), y_\n",
    "def gen_samples_uniform(xcp, xcs, xp, nw, verbose=False):  # Weight testing samples (word lists) uniformly\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    for i in range(len(xcp)):\n",
    "        x, y, sqlen = [], [], []\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        for m in range(nw):\n",
    "            prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "            cat_ix = np.random.randint(len(cp))\n",
    "            x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "            sqlen.append(len(x_))\n",
    "            j += 1\n",
    "            if verbose and j % 100 == 0:\n",
    "                sys_print(\"\\rDone: \" + str(j))\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        sqlens.append(sqlen)\n",
    "    if verbose: sys_print(\"\\rDone: \" + str(j) + \", finished!\\n\")\n",
    "    return xs, ys, sqlens\n",
    "def gen_samples(xcp, xcs, xp, n):  # Maximise training batch diversity by randomly sampling the word lists\n",
    "    xs, ys, sqlens, j = [], [], [], 0\n",
    "    n_sets = len(xcp)\n",
    "    for m in range(n):\n",
    "        i = np.random.randint(n_sets)\n",
    "        cp, cs, p = xcp[i], xcs[i], xp[i]\n",
    "        prmt, typesof = lprompts_encoded[np.random.randint(len(lprompts_encoded))]\n",
    "        cat_ix = np.random.randint(len(cp))\n",
    "        x_, y_ = gen_truncated_list(np.hstack([prmt, cp[cat_ix] if typesof else cs[cat_ix]]), p)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "        sqlens.append(len(x_))\n",
    "    return xs, ys, sqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Done: 50, finished!\n"
     ]
    }
   ],
   "source": [
    "N_test = int(test_set_frac * N_wordlists)\n",
    "N_train = N_wordlists - N_test\n",
    "# test_idx = np.random.choice(N_wordlists, N_test, replace=False)\n",
    "test_idx = np.array([3, 7])\n",
    "cats_e_test, cats_sing_e_test = [cats_e[i] for i in test_idx], [cats_sing_e[i] for i in test_idx]\n",
    "phrases_e_test = [phrases_e[i] for i in test_idx]\n",
    "train_idx = [i for i in range(N_wordlists) if i not in test_idx]\n",
    "cats_e_train, cats_sing_e_train = [cats_e[i] for i in train_idx], [cats_sing_e[i] for i in train_idx]\n",
    "phrases_e_train = [phrases_e[i] for i in train_idx]\n",
    "test_cats = [cats[i][0] for i in test_idx]\n",
    "test_xs, test_ys, test_sqlens = gen_samples_uniform(cats_e_test, cats_sing_e_test, phrases_e_test, sample_test_n, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ld((test_xs, test_ys, test_sqlens), \"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xs, test_ys, test_sqlens = load_ld(\"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define next batch function\n",
    "def adapt_form(xs, ys, sqlens):\n",
    "    max_len = max(sqlens)\n",
    "    xs = pt.tensor(np.vstack([np.pad(x, (0, max_len - len(x)), constant_values=50256) for x in xs]))\n",
    "    ys = pt.tensor(np.vstack(ys))\n",
    "    sqlens = pt.tensor(np.asarray(sqlens))\n",
    "    return xs, ys, sqlens\n",
    "def next_batch(sz):\n",
    "    global cats_e_train, cats_sing_e_train, phrases_e_train\n",
    "    xs, ys, sqlens = gen_samples(cats_e_train, cats_sing_e_train, phrases_e_train, sz)\n",
    "    return adapt_form(xs, ys, sqlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "create_folder(\"models\")\n",
    "create_folder(\"models/pretrained\")\n",
    "create_folder(\"models/pretrained/GPT2SeqClas\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2-large',\n",
    "    output_hidden_states=True, output_attentions=True, \n",
    "    cache_dir=\"models/pretrained/GPT2SeqClas\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.to(device)\n",
    "print(\"GPT2 loaded\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "llayer = nn.Linear(model.config.n_embd, len(tokenizer), bias=False)\n",
    "# softmax = nn.Softmax()\n",
    "bcewl_loss = nn.BCEWithLogitsLoss()\n",
    "# nll_loss = nn.NLLLoss()\n",
    "# kl_loss = nn.KLDivLoss()\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_sched_warmup, num_training_steps=N_train_batches)\n",
    "def train_step():\n",
    "    global model, llayer, bcewl_loss, optimizer, scheduler\n",
    "    x_batch, y_batch, sqlens_batch = next_batch(batch_size)\n",
    "    x_batch, y_batch, sqlens_batch = x_batch.to(device), y_batch.to(device), sqlens_batch.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    outputs = model(x_batch.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens_batch - 1, 1).repeat((1, model.config.n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1], 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    \n",
    "#     logsofts = pt.log(softmax(logits))\n",
    "    loss = bcewl_loss(logits, y_batch.float())\n",
    "    correct = pt.mean((y_batch[pt.arange(batch_size), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "    loss_, correct_ = loss.detach().cpu().numpy(), correct.detach().cpu().numpy()\n",
    "    \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss_, correct_\n",
    "\n",
    "def inference(x, sqlens):\n",
    "    global model, llayer\n",
    "    \n",
    "    x, sqlens = x.to(device), sqlens.to(device)\n",
    "    outputs = model(x.long())\n",
    "    out_idx = pt.unsqueeze(pt.unsqueeze(sqlens - 1, 1).repeat((1, model.config.n_embd)), 1).type(pt.int64)\n",
    "    outs = pt.gather(outputs[2][-1], 1, out_idx).squeeze(1)\n",
    "    logits = llayer(outs)\n",
    "    return logits\n",
    "def eval_test(x, y, sqlens):\n",
    "    global bcewl_loss\n",
    "    \n",
    "    with pt.no_grad():\n",
    "        logits = inference(x, sqlens)\n",
    "        loss = bcewl_loss(logits, y.float())\n",
    "        correct = pt.mean((y[pt.arange(x.shape[0]), pt.argmax(logits, axis=1)] > (lidstone_e / N_tokens)).float())\n",
    "        loss_, correct_ = loss.numpy(), correct.numpy()\n",
    "    return loss_, correct_\n",
    "\n",
    "# top_next = [self.tokenizer.decode(i.item()).strip() for i in probs.topk(k)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "best_acc, best_loss = 0, np.inf\n",
    "best_acc_idx = -1\n",
    "create_folder(\"models\")\n",
    "create_folder(\"model_logs\")\n",
    "create_folder(\"models/\" + model_name)\n",
    "graphs_folder = \"graphs\"\n",
    "create_folder(graphs_folder)\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 126 : 0.0 0.0 loss: 0.68156487 0.68155503 Best: 0 inf idx: -1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss, accuracy: 0.68152386, 0.0 @ batch 147 (1176 samples) complete. "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-863-5ff61b84ca1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0miterate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-862-0233a8c8d450>\u001b[0m in \u001b[0;36miterate_training\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mb_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0msys_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\rLoss, accuracy: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                       \u001b[1;34m' @ batch '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' ('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' samples) complete. '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-860-b0d919876998>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "iterate_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(verbose=True):\n",
    "    global model, batch_i, best_acc, best_loss, best_acc_idx, train_loss, train_accuracy, test_loss, test_accuracy\n",
    "    \n",
    "    model.train()\n",
    "    iter_loss, iter_accuracy, b_no_inp = [], [], 0\n",
    "    while batch_i < N_train_batches:\n",
    "        batch_i += 1\n",
    "        gc.collect()\n",
    "        b_loss, b_accuracy = train_step()\n",
    "        if verbose:\n",
    "            sys_print('\\rLoss, accuracy: ' + str(np.mean(b_loss)) + ', ' + str(np.mean(b_accuracy)) + \\\n",
    "                      ' @ batch '+ str(batch_i) + ' (' + str(batch_i * batch_size) + ' samples) complete. ')\n",
    "        iter_loss.append(b_loss)\n",
    "        iter_accuracy.append(b_accuracy)\n",
    "        \n",
    "        if (batch_i - 1) % log_period_batches == 0:  # Test on test set\n",
    "            loss, accuracy = [], []\n",
    "            for i in range(N_test):\n",
    "                test_X, test_Y, test_Sqlens = adapt_form(test_xs[i], test_ys[i], test_sqlens[i])\n",
    "                feed_batches = [range(len(test_X))[i * bsz:(i + 1) * bsz] for i in range((len(test_X) // bsz) + 1)]\n",
    "                ls, cs = zip(*[eval_test(test_X[inds], test_Y[inds], test_Sqlens[inds]) for inds in feed_batches])\n",
    "                loss.append(np.mean(ls))\n",
    "                accuracy.append(np.mean(cs))\n",
    "                print('\\n' + test_cats[i] + ': ' + str(loss[-1]) + ', ' + str(accuracy[-1]))\n",
    "            \n",
    "            test_l, test_a = np.mean(loss), np.mean(accuracy)\n",
    "            test_loss.append(test_l)\n",
    "            test_accuracy.append(test_a)\n",
    "            train_l, train_a = np.mean(iter_loss), np.mean(iter_accuracy)\n",
    "            train_loss.append(train_l)\n",
    "            train_accuracy.append(train_a)\n",
    "            iter_loss, iter_accuracy = [], []\n",
    "            \n",
    "            val_a = 0\n",
    "            if test_a > best_acc:      # Save best accuracy model\n",
    "                best_acc = test_a\n",
    "                best_loss = test_l\n",
    "                best_acc_idx = batch_i // log_period_batches\n",
    "                pt.save({\"model\": model.state_dict(),\n",
    "                         \"llayer\": llayer.state_dict(),\n",
    "#                          \"softrmax\": softrmax.state_dict(),\n",
    "                         \"bcewl_loss\": bcewl_loss.state_dict(),\n",
    "#                          \"nll_loss\": nll_loss.state_dict(),\n",
    "#                          \"kl_loss\": kl_loss.state_dict(),\n",
    "                         \"optimizer\": optimizer.state_dict(),\n",
    "                         \"scheduler\": scheduler.state_dict(),\n",
    "                         }, \"./models/\" + model_name + '/' + model_name)\n",
    "                b_no_inp = 0\n",
    "            else:\n",
    "                b_no_inp += log_period_batches\n",
    "                \n",
    "            if verbose:\n",
    "                clear_output()\n",
    "                print(\"Batch\", batch_i, ':', train_a, test_a, \"loss:\", train_l, test_l, \\\n",
    "                      \"Best:\", best_acc, best_loss, 'idx:', best_acc_idx)\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(16, 5)\n",
    "                g = fig.add_subplot(1,2,1)\n",
    "                g.grid()\n",
    "                g.plot(train_accuracy, label='train acc')\n",
    "                g.plot(test_accuracy, label='test acc')\n",
    "                g.legend(loc='lower right')\n",
    "#                 g.axhline(y=0.714, ls='--', color='grey')\n",
    "\n",
    "                g = fig.add_subplot(1,2,2)\n",
    "                g.grid()\n",
    "                g.plot(train_loss, label='train loss')\n",
    "                g.plot(test_loss, label='test loss')\n",
    "                g.legend(loc='upper right')\n",
    "\n",
    "                save_ld((train_accuracy, test_accuracy, train_loss, test_loss),\n",
    "                        \"model_logs/\" + model_name + '_log_latest', pad=False)\n",
    "                plt.savefig(graphs_folder + '/' + model_name + \"_curve_latest\" + '.pdf', format='pdf')\n",
    "                plt.show()\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:234: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "checkpoint = pt.load(\"./models/\" + model_name + '/' + model_name)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "llayer.load_state_dict(checkpoint['llayer'])\n",
    "bcewl_loss.load_state_dict(checkpoint['bcewl_loss'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < pt.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = pt.sort(logits, descending=True)\n",
    "        cumulative_probs = pt.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_next_token(sent, top_k=-1, top_p=0.9, temperature=1.0):\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    x = pt.tensor([tokens])\n",
    "    logits = inference(x, pt.tensor([len(tokens)]))[0]\n",
    "    logits /= temperature\n",
    "    logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    token = pt.multinomial(probs, 1).numpy()[0]\n",
    "    tokens += [token]\n",
    "    sent = tokenizer.decode(tokens)\n",
    "    print(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A list of types of drink: coffee, water, tea, coke, lemonade, milkshake,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A list of types of drink: coffee, water, tea, coke, lemonade, milkshake, changed\n"
     ]
    }
   ],
   "source": [
    "input_sentence = append_next_token(input_sentence, top_k=-1, top_p=0.3, temperature=4.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
